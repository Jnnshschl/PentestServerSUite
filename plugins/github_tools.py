import logging as logger
from datetime import datetime
from json import loads
from os import makedirs
from pathlib import Path
from re import match

from colorama import Fore
from patoolib import extract_archive
from requests import get

from pssutil.cmdutils import CmdLenValidator
from pssutil.fileutils import load_json, save_json

TAG = f"[{Fore.LIGHTGREEN_EX}GithubTools{Fore.RESET}]"

GITHUB_PATTERN = "https://api.github.com/repos/{}/releases/latest"
GITHUB_DATETIME_PATTERN = "%Y-%m-%dT%H:%M:%SZ"

GITHUB_TOOLS_TO_DOWNLOAD = Path("./conf/github_tools.json")
GITHUB_TOOLS = None

logger.getLogger("patool").setLevel(logger.ERROR)
logger.getLogger("subprocess").setLevel(logger.ERROR)


def add_cmd(pm, cmd):
    ghtools_handler = cmd.add(["githubtools"])
    ghtools_handler.add(["update"], lambda _: update_tools(pm))
    ghtools_handler.add(
        ["add"],
        CmdLenValidator(
            lambda args: add_tool(pm, args),
            4,
            ["category", "name", "github url or author/repo", "regex patterns"],
        ),
    )


def add_tool(pm, args):
    global GITHUB_TOOLS
    platform = args[0]
    name = args[1]
    url = args[2]
    patterns = args[3:]

    GITHUB_TOOLS = load_json(GITHUB_TOOLS_TO_DOWNLOAD)

    if GITHUB_TOOLS is not None:
        if "github.com/" in url:
            url = url.split("github.com/")[1]

        if "/" in url:
            url_parts = url.split("/")

            if len(url_parts) > 1:
                url = f"{url_parts[0]}/{url_parts[1]}"
        else:
            logger.error(f'{TAG} Invalid url, should be: "author/repository" or "https://github.com/carlospolop/PEASS-ng"')
            return

        if platform not in GITHUB_TOOLS:
            GITHUB_TOOLS[platform] = {}

        if name not in GITHUB_TOOLS[platform]:
            GITHUB_TOOLS[platform][name] = {}

        if url not in GITHUB_TOOLS[platform][name]:
            GITHUB_TOOLS[platform][name][url] = {"patterns": [], "updated_at": {}}

        for pattern in patterns:
            GITHUB_TOOLS[platform][name][url]["patterns"].append(pattern)

        GITHUB_TOOLS[platform][name][url]["patterns"] = list(set(GITHUB_TOOLS[platform][name][url]["patterns"]))
        update_tool(pm, platform, name, GITHUB_TOOLS[platform][name])
        save_json(GITHUB_TOOLS_TO_DOWNLOAD, GITHUB_TOOLS)


def update_tools(pm):
    global GITHUB_TOOLS
    GITHUB_TOOLS = load_json(GITHUB_TOOLS_TO_DOWNLOAD)
    logger.info(f"{TAG} Updating All Tools")

    if GITHUB_TOOLS is not None:
        for platform, tool in GITHUB_TOOLS.items():
            for subfolder, downloads in tool.items():
                update_tool(pm, platform, subfolder, downloads)
    else:
        GITHUB_TOOLS = {}


def update_tool(pm, platform, subfolder, downloads):
    filepath = Path(pm.cfg.serve_folder, f"{platform}/{subfolder}/")

    if not filepath.exists():
        makedirs(filepath)

    for url, attributes in downloads.items():
        gh_url = GITHUB_PATTERN.format(url)
        resp = get(gh_url)

        if resp.status_code == 200:
            gh_json = loads(resp.text)

            if "assets" in gh_json:
                for asset in gh_json["assets"]:
                    for pattern in attributes["patterns"]:
                        if "name" in asset and "browser_download_url" in asset and "updated_at" in asset:
                            if match(pattern, asset["name"]):
                                asset_id = asset["name"]
                                asset_path = Path(filepath, asset["name"])

                                if (
                                    asset_id not in attributes["updated_at"]
                                    or not asset_path.exists()
                                    or datetime.strptime(attributes["updated_at"][asset_id], GITHUB_DATETIME_PATTERN)
                                    < datetime.strptime(asset["updated_at"], GITHUB_DATETIME_PATTERN)
                                ):
                                    logger.info(f"{TAG} Updating {asset['name']}: {asset_path}")

                                    with get(asset["browser_download_url"], stream=True) as resp_asset:
                                        if resp_asset.status_code == 200:
                                            with open(asset_path, "wb+") as f:
                                                for chunk in resp_asset.iter_content(chunk_size=8192):
                                                    f.write(chunk)

                                            if asset_path.suffix in [".zip", ".7z", ".rar", ".tar", ".gz"]:
                                                try:
                                                    logger.info(f"{TAG} Auto-Extracting {asset_path}")
                                                    extract_archive(str(asset_path), interactive=False, outdir=str(asset_path.parent))
                                                except Exception as ex:
                                                    logger.warn(f"{TAG} Extraction failed: {ex}")

                                            attributes["updated_at"][asset_id] = asset["updated_at"]
                                            logger.info(f"{TAG} Sucessfully updated {asset['name']} to {asset['updated_at']}")
                                            save_json(GITHUB_TOOLS_TO_DOWNLOAD, GITHUB_TOOLS)
                                        else:
                                            logger.error(
                                                f"{TAG} Download failed \"{asset['browser_download_url']}\": [{resp_asset.status_code}]: {resp_asset.text}"
                                            )
                        else:
                            logger.error(f"{TAG} Parsing GithubAPI response failed for {asset['name']}: invalid asset definition")
            else:
                logger.error(f"{TAG} Parsing GithubAPI response failed for {asset['name']}: no assets found")
        else:
            logger.error(f'{TAG} Fetching GithubAPI failed "{gh_url}": [{resp.status_code}]: {resp.text}')
