import base64
import json
import logging as logger
import re
import warnings
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import parse_qs, urljoin, urlparse

import requests
import requests.adapters
from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning
from colorama import Fore

from pssutil.cmdutils import CmdLenValidator

logger.getLogger("bs4").setLevel(logger.ERROR)
warnings.filterwarnings("ignore", module="html.parser")
warnings.filterwarnings("ignore", category=MarkupResemblesLocatorWarning)
warnings.filterwarnings("ignore", module="bs4")

TAG = f"[{Fore.LIGHTYELLOW_EX}WebScrape{Fore.RESET}]"

URL_REGEX = re.compile(r"[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b[-a-zA-Z0-9()@:%_\+.~#?&//=]*")
HTTP_URL_REGEX = re.compile(r"https?:\/\/www\.?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b[-a-zA-Z0-9()@:%_\+.~#?&//=]*")
BASE64_REGEX = re.compile(r"\b[A-Za-z0-9+/]+={0,2}\b")

SENSITIVE_KEYWORDS = {
    "admin",
    "login",
    "password",
    "secret",
    "oauth",
    "auth",
    "access_token",
    "api_key",
    "key",
}

URL_CACHE = {}
URL_SESSION = requests.Session()
URL_ADAPTER = requests.adapters.HTTPAdapter(pool_maxsize=2048)
URL_SESSION.mount("http://", URL_ADAPTER)
URL_SESSION.mount("https://", URL_ADAPTER)


def add_cmd(pm, cmd):
    webscrape_handler = cmd.add(["webscrape"])
    webscrape_handler.add(["all"], CmdLenValidator(scrape_website, 1, ["url", "maxdepth"]))
    webscrape_handler.add(["urls"], CmdLenValidator(scrape_website, 1, ["url", "maxdepth"]))


def scrape_website(args):
    scrape_urls(args)


def scrape_urls(args):
    global URL_CACHE
    URL_CACHE.clear()

    max_depth = int(args[1]) if len(args) > 1 else 4
    urls = get_unique_urls_with_status(args[0], max_depth)

    logger.info(f"{TAG} Calculation URL scores...")
    highlights = urls_build_highlight_list(urls)
    print(json.dumps(highlights, indent=2))
    logger.info(f"{TAG} Found a total of {len(highlights)} URLs")


def decode_base64_and_search_urls(text):
    decoded_text = ""

    for base64_string in BASE64_REGEX.findall(text):
        try:
            decoded_bytes = base64.b64decode(base64_string)
            decoded_text += decoded_bytes.decode("utf-8")
        except Exception:
            pass

    return HTTP_URL_REGEX.findall(decoded_text)


def get_unique_urls_with_status(base_url, max_depth=4, current_depth=0, visited_urls=None):
    if current_depth > max_depth:
        return visited_urls

    if visited_urls is None:
        visited_urls = set()

    response = proxy_get_request(base_url)

    if not response:
        return visited_urls

    soup = BeautifulSoup(response.text, "html.parser")

    links = [link["href"] for link in soup.find_all("a", href=True) if not link["href"].startswith(("#", "mailto:"))]
    script_urls = [script["src"] for script in soup.find_all("script", src=True)]
    style_urls = [style["href"] for style in soup.find_all("link", rel="stylesheet", href=True)]
    # metadata_urls = [meta["content"] for meta in soup.find_all("meta", content=True) if HTTP_URL_REGEX.match(meta["content"])]

    text_urls = HTTP_URL_REGEX.findall(response.text)
    base64_urls = decode_base64_and_search_urls(response.text)

    # print(f"links: {len(links)}\n{links}")
    # print(f"script_urls: {len(script_urls)}\n{script_urls}")
    # print(f"style_urls: {len(style_urls)}\n{style_urls}")
    # print(f"metadata_urls: {len(metadata_urls)}\n{metadata_urls}")
    # print(f"text_urls: {len(text_urls)}\n{text_urls}")
    # print(f"base64_urls: {len(base64_urls)}\n{base64_urls}")

    found_urls = set(links + script_urls + style_urls + text_urls + base64_urls)

    url_parsed = urlparse(base_url)

    if not url_parsed.path:
        robots_txt = get_robots_txt(base_url)
        disallowed_urls = parse_robots_txt(robots_txt)
        found_urls.update(disallowed_urls)
        # print(f"disallowed_urls: {len(disallowed_urls)}\n{disallowed_urls}")

    new_urls = []

    for url in found_urls:
        if not url.startswith("http"):
            url = urljoin(base_url, url)

        if urlparse(url).netloc.endswith(url_parsed.netloc):
            new_urls.append(url)

    new_urls = set(new_urls) - visited_urls

    if new_urls:
        visited_urls.update(new_urls)
        logger.info(f'{TAG} [{current_depth}/{max_depth}] {len(visited_urls)} URLs (+{len(new_urls)}) @ "{base_url}"')

        with ThreadPoolExecutor(max_workers=8) as executor:
            for new_url in new_urls:
                executor.submit(get_unique_urls_with_status, new_url, max_depth, current_depth + 1, visited_urls)

    return visited_urls


def get_robots_txt(base_url):
    robots_url = urljoin(base_url, "/robots.txt")
    response = proxy_get_request(robots_url)
    return response.text if response else ""


def parse_robots_txt(robots_txt):
    disallowed_urls = []

    for line in robots_txt.split("\n"):
        if line.strip().startswith("Disallow:"):
            disallowed_urls.append(line.strip().split(":")[1].strip())

    return disallowed_urls


def rate_url(url):
    parsed_url = urlparse(url)
    lower_url = url.lower()
    score = 0

    if parsed_url.netloc.count(".") > 1:
        score += 3

    query_params = parse_qs(parsed_url.query)
    score += len(query_params)

    for keyword in SENSITIVE_KEYWORDS:
        if keyword in lower_url or keyword in query_params:
            score += 10
            break

    response = proxy_get_request(url)

    if response:
        status_code = response.status_code
        if status_code == 401:
            score += 8
        elif status_code == 403:
            score += 5
        elif status_code == 500:
            score += 5
    return score


def proxy_get_request(url, timeout=2):
    global URL_CACHE
    global URL_SESSION

    if url in URL_CACHE:
        return URL_CACHE[url]
    try:
        response = URL_SESSION.get(url, timeout=timeout)
        URL_CACHE[url] = response

        if response.status_code != 404:
            return response
    except requests.RequestException:
        pass
    return None


def urls_build_highlight_list(urls):
    return sorted(urls, key=lambda url: rate_url(url), reverse=True)
