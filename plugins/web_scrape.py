import base64
import logging as logger
import re
import warnings
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import parse_qs, urljoin, urlparse

import requests
import requests.adapters
from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning
from colorama import Fore
from tqdm import tqdm

from pssutil.cmdutils import CmdLenValidator

logger.getLogger("bs4").setLevel(logger.ERROR)
warnings.filterwarnings("ignore", module="html.parser")
warnings.filterwarnings("ignore", category=MarkupResemblesLocatorWarning)
warnings.filterwarnings("ignore", module="bs4")

TAG = f"[{Fore.LIGHTYELLOW_EX}WebScrape{Fore.RESET}]"

URL_REGEX = re.compile(r"[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b[-a-zA-Z0-9()@:%_\+.~#?&//=]*")
HTTP_URL_REGEX = re.compile(r"https?:\/\/www\.?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b[-a-zA-Z0-9()@:%_\+.~#?&//=]*")
BASE64_REGEX = re.compile(r"\b[A-Za-z0-9+/]+={0,2}\b")

SENSITIVE_KEYWORDS = {
    "admin",
    "login",
    "password",
    "secret",
    "oauth",
    "auth",
    "access_token",
    "api_key",
    "key",
}

URL_CACHE = {}
URL_SESSION = requests.Session()
URL_ADAPTER = requests.adapters.HTTPAdapter(pool_maxsize=2048)
URL_SESSION.mount("http://", URL_ADAPTER)
URL_SESSION.mount("https://", URL_ADAPTER)


def add_cmd(pm, cmd):
    webscrape_handler = cmd.add(["webscrape"])
    webscrape_handler.add(["all"], CmdLenValidator(scrape_website, 1, ["url", "maxdepth"]))
    webscrape_handler.add(["urls"], CmdLenValidator(scrape_website, 1, ["url", "maxdepth"]))


def scrape_website(args):
    scrape_urls(args)


def scrape_urls(args):
    global URL_CACHE
    URL_CACHE.clear()

    max_depth = int(args[1]) if len(args) > 1 else 4
    urls = get_unique_urls_with_status(args[0], max_depth)

    logger.info(f"{TAG} Calculation URL scores...")
    highlights = urls_build_highlight_list(urls)
    pretty_print_dict(highlights)
    logger.info(f"{TAG} Found a total of {len(highlights)} URLs")


def decode_base64_and_search_urls(text):
    decoded_text = ""

    for base64_string in BASE64_REGEX.findall(text):
        try:
            decoded_bytes = base64.b64decode(base64_string)
            decoded_text += decoded_bytes.decode("utf-8")
        except Exception:
            pass

    return HTTP_URL_REGEX.findall(decoded_text)


def get_unique_urls_with_status(base_url, max_depth=4, current_depth=0, visited_urls=None):
    if current_depth > max_depth:
        return visited_urls

    if visited_urls is None:
        visited_urls = set()

    response = proxy_get_request(base_url)

    if not response:
        return visited_urls

    soup = BeautifulSoup(response.text, "html.parser")

    links = [link["href"] for link in soup.find_all("a", href=True) if not link["href"].startswith(("#", "mailto:"))]
    script_urls = [script["src"] for script in soup.find_all("script", src=True)]
    style_urls = [style["href"] for style in soup.find_all("link", rel="stylesheet", href=True)]
    # metadata_urls = [meta["content"] for meta in soup.find_all("meta", content=True) if HTTP_URL_REGEX.match(meta["content"])]

    text_urls = HTTP_URL_REGEX.findall(response.text)
    base64_urls = decode_base64_and_search_urls(response.text)

    found_urls = set(links + script_urls + style_urls + text_urls + base64_urls)

    base_url_parsed = urlparse(base_url)

    if not base_url_parsed.path or base_url_parsed.path == "/":
        robots_txt = get_robots_txt(base_url)
        disallowed_urls = parse_robots_txt(robots_txt)
        found_urls.update(disallowed_urls)

    new_urls = []

    for url in found_urls:
        try:
            url_parsed = urlparse(url)

            if not url_parsed.scheme.startswith("http"):
                url = urljoin(base_url, url)
                url_parsed = urlparse(url)

            if url_parsed.netloc.endswith(base_url_parsed.netloc):
                new_urls.append(url)
        except:
            pass

    new_urls = set(new_urls) - visited_urls

    if new_urls:
        visited_urls.update(new_urls)
        logger.info(f'{TAG} [{current_depth}/{max_depth}] ({len(visited_urls)} URLs) +{len(new_urls):4} @ "{base_url}"')

        with ThreadPoolExecutor(max_workers=8) as executor:
            for new_url in new_urls:
                executor.submit(get_unique_urls_with_status, new_url, max_depth, current_depth + 1, visited_urls)

    return visited_urls


def get_robots_txt(base_url):
    robots_url = urljoin(base_url, "/robots.txt")
    response = proxy_get_request(robots_url)
    return response.text if response else ""


def parse_robots_txt(robots_txt):
    disallowed_urls = []

    for line in robots_txt.split("\n"):
        if line.strip().startswith("Disallow:"):
            disallowed_urls.append(line.strip().split(":")[1].strip())

    return disallowed_urls


def rate_url(url):
    parsed_url = urlparse(url)
    lower_url = url.lower()
    score = 0

    if parsed_url.netloc.count(".") > 1:
        score += 3

    query_params = parse_qs(parsed_url.query)
    score += len(query_params)

    for keyword in SENSITIVE_KEYWORDS:
        if keyword in lower_url or keyword in query_params:
            score += 10

    response = proxy_get_request(url)

    if response:
        status_code = response.status_code
        if status_code == 401:
            score += 8
        elif status_code == 403:
            score += 5
        elif status_code == 500:
            score += 5
    return score


def proxy_get_request(url, timeout=2):
    global URL_CACHE
    global URL_SESSION

    if url in URL_CACHE:
        return URL_CACHE[url]

    try:
        response = URL_SESSION.get(url, timeout=timeout)
        URL_CACHE[url] = response

        if response.status_code != 404:
            return response
    except requests.RequestException:
        pass

    return None


def urls_build_highlight_list(urls):
    rated_urls = {}

    with tqdm(
        ascii=True,
        desc=f"{Fore.YELLOW}Rating URL's{Fore.RESET}",
        unit=f" {Fore.YELLOW}urls{Fore.RESET}",
        total=len(urls),
    ) as pbar:

        def rate_url_int(u, p):
            rated_urls[u] = rate_url(u)
            p.update()

        with ThreadPoolExecutor(max_workers=16) as executor:
            for url in urls:
                executor.submit(rate_url_int, url, pbar)

    # sort by name first, then by score
    rated_urls = {k: v for k, v in sorted(rated_urls.items(), key=lambda item: item[0])}
    return {k: v for k, v in sorted(rated_urls.items(), key=lambda item: item[1])}


def interpolate_color(value, min_val, max_val):
    div = (max_val - min_val)
    ratio = (value - min_val) / div if div > 0.0 else 0.0
    r = int(255 * (1.0 - ratio))
    g = int(255 * min(1.0, ratio + 0.5))
    return f"\033[38;2;0;{g};{r}m"


def pretty_print_dict(data):
    values = list(data.values())
    min_val = min(values)
    max_val = max(values)

    for key, value in data.items():
        print(f"{interpolate_color(value, min_val, max_val)}{key}{Fore.RESET}")
