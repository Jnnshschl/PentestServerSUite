import asyncio
import ipaddress
import json
import logging as logger
import os
import re
import shlex
import socket
import subprocess
import tempfile
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from urllib.parse import parse_qs, unquote, urlencode, urlparse, urlunparse
from urllib.request import urlretrieve

import nmap
from colorama import Fore
from python_hosts import Hosts, HostsEntry
from tornado import gen, httpclient, ioloop
from tqdm import tqdm
from xxhash import xxh64_intdigest

from pssutil.cmdutils import CmdLenValidator

TAG = f"[{Fore.LIGHTYELLOW_EX}ReconBuddy{Fore.RESET}]"

WHATWEB_PLUGIN_BLACKLIST = ["Country", "IP"]

WORDLISTS_FOLDER = Path("./wordlists")
WORDLISTS = {
    "vhosts": "https://raw.githubusercontent.com/danielmiessler/SecLists/master/Discovery/DNS/subdomains-top1million-5000.txt",
    "lfi": "https://raw.githubusercontent.com/carlospolop/Auto_Wordlists/main/wordlists/file_inclusion_linux.txt",
}

BANNER_REGEXES = {
    "HTTP": [
        re.compile(r"^HTTP\/.+\ [0-9]{1,3}\ "),
    ],
    "SSH": [
        re.compile(r"^SSH-[0-9]\.[0-9]-.+"),
    ],
}


def add_cmd(pm, cmd):
    rbuddy_handler = cmd.add(["rbuddy", "reconbuddy"])
    rbuddy_handler.add(["scan"], CmdLenValidator(init_recon_scan, 1, ["hostname or ipadress"]))
    rbuddy_handler.add(["bannergrab"], CmdLenValidator(init_banner_grab, 2, ["hostname or ipadress", "port"]))
    rbuddy_handler.add(["identifyservice"], CmdLenValidator(init_service_identify, 2, ["hostname or ipadress", "port"]))

    lfi_handler = rbuddy_handler.add(["wordlists"])
    lfi_handler.add(["update"], update_wordlists)

    lfi_handler = rbuddy_handler.add(["lfi"])
    lfi_handler.add(["scan"], CmdLenValidator(init_http_lfi_scan, 1, ["url with parameters"]))


def update_wordlists(args):
    for filename, url in WORDLISTS.items():
        download_wordlist(url, Path(WORDLISTS_FOLDER, filename))


def download_wordlist(url, path):
    try:
        if not WORDLISTS_FOLDER.exists():
            os.makedirs(WORDLISTS_FOLDER)

        logger.info(f'{TAG} Downloading "{Fore.LIGHTCYAN_EX}{path}{Fore.RESET}": {Fore.LIGHTCYAN_EX}{url}{Fore.RESET}')

        if path.exists():
            os.remove(path)

        urlretrieve(url, path)
    except Exception as ex:
        logger.error(f'{TAG} Failed to download wordlist "{Fore.LIGHTCYAN_EX}{path}{Fore.RESET}": {ex}')


def get_wordlist(name):
    try:
        if name in WORDLISTS:
            path = Path(WORDLISTS_FOLDER, name)

            if not path.exists():
                download_wordlist(WORDLISTS[name], path)

            with open(path) as wl:
                return [x.strip() for x in wl.readlines()]
    except Exception as ex:
        logger.error(f'{TAG} Failed to read wordlist "{Fore.LIGHTCYAN_EX}{path}{Fore.RESET}": {ex}')
    return []


def init_recon_scan(args):
    try:
        host = args[0]
        logger.info(f"{TAG} Scanning: {host}")
        nmap_scanner = nmap.PortScanner()

        ports = args[1] if len(args) > 1 else "-"
        nmap_tcp_scan = nmap_scanner.scan(host, arguments=f"-Pn -sV --min-rate 6000 -p{ports}")

        if "scan" in nmap_tcp_scan and host in nmap_tcp_scan["scan"]:
            # logger.info(f"{TAG} {nmap_tcp_scan['scan'][host]}")

            services = {}

            if "tcp" in nmap_tcp_scan["scan"][host] and nmap_tcp_scan["scan"][host]["tcp"]:
                for port, data in nmap_tcp_scan["scan"][host]["tcp"].items():
                    service_name = f'{data.get("product", "n/a")} {data.get("version", "n/a")}'
                    logger.info(
                        f'{TAG} {Fore.LIGHTCYAN_EX}{str(port).ljust(5)}{Fore.RESET} : {data.get("state", "unknown")} [{Fore.YELLOW}{data.get("name", "NONE").upper()}{Fore.RESET}] ({Fore.YELLOW}{service_name}{Fore.RESET})'
                    )

                    def identified_service(services, service, port, print_msg: bool = False):
                        if service not in services:
                            services[service] = []

                        services[service].append(port)

                        if print_msg:
                            logger.info(
                                f"{TAG} >> {Fore.LIGHTGREEN_EX}Service identified as{Fore.RESET}: {Fore.YELLOW}{service}{Fore.RESET}"
                            )

                    # nmap service recognition
                    if "name" in data and data["name"] and data["name"].lower() != "unknown":
                        if "version" in data and data["version"]:
                            run_exploitdb_lookup(service_name)

                        identified_service(services, data["name"], port)

                    # try to indetify unknown services
                    else:
                        service = try_identify_service(host, port)

                        if service:
                            identified_service(services, service, port, True)

                for service, ports in services.items():
                    service = service.upper()

                    if service == "HTTP":
                        [http_recon(host, port) for port in ports]
            else:
                logger.warn(f"{TAG} No open TCP ports found...")
        else:
            logger.error(f"{TAG} Failed to run nmap scan...")
    except Exception as ex:
        logger.error(f"{TAG} Failed to run recon scan: {ex}")


def init_banner_grab(args):
    try:
        identify_by_banner_grabbing(args[0], int(args[1]), True)
    except Exception as ex:
        logger.error(f"{TAG} Failed to run banner grab: {ex}")


def init_service_identify(args):
    try:
        service = try_identify_service(args[0], int(args[1]))

        if service:
            logger.info(f"{TAG} >> {Fore.LIGHTGREEN_EX}Service identified as{Fore.RESET}: {Fore.YELLOW}{service}{Fore.RESET}")
        else:
            logger.info(f"{TAG} >> {Fore.LIGHTRED_EX}Service could not be identified{Fore.RESET}")
    except Exception as ex:
        logger.error(f"{TAG} Failed to identify service: {ex}")


def try_identify_service(host: str, port: int, print_banner: bool = False):
    try:
        service = identify_by_banner_grabbing(host, port, print_banner)

        if service:
            return service
        else:
            indentifiers = {"HTTP": [indentify_http_service]}

            for service, identifier_fns in indentifiers.items():
                for identifier_fn in identifier_fns:
                    if identifier_fn(host, port):
                        return service
    except Exception as ex:
        logger.error(f"{TAG} Failed to identify service: {ex}")

    return None


def buid_url(scheme, host, port):
    return f"{scheme}://{host}:{port}"


def run_cmd(cmd):
    return subprocess.run(shlex.split(cmd), capture_output=True).stdout


def run_exploitdb_lookup(service_name):
    try:
        searchsploit_result = json.loads(run_cmd(f"searchsploit -w --json '{service_name}'"))

        if "RESULTS_EXPLOIT" in searchsploit_result and searchsploit_result["RESULTS_EXPLOIT"]:
            result_str = f"{Fore.RESET}\n".join(
                [f"{Fore.LIGHTYELLOW_EX}{x['Title']}{Fore.RESET}: {x['URL']}" for x in searchsploit_result["RESULTS_EXPLOIT"]]
            )
            result_str = re.sub("- ", f"{Fore.RESET} - {Fore.LIGHTRED_EX}", result_str)
            logger.info(f"{TAG} >> {Fore.LIGHTGREEN_EX}Exploits available{Fore.RESET}:\n{result_str}")
        else:
            logger.info(f"{TAG} >> {Fore.WHITE}No exploits found...{Fore.RESET}")
    except Exception as ex:
        logger.error(f"{TAG} Failed to search for exploits: {ex}")


def run_whatweb(url):
    _, path = tempfile.mkstemp(suffix=".json")

    try:
        run_cmd(f"whatweb -q --log-json '{path}' '{url}'")

        with open(path) as tmp_file:
            whatweb_results = json.loads(tmp_file.read())

            for whatweb_result in whatweb_results:
                if "target" in whatweb_result and "plugins" in whatweb_result:
                    whatweb_plugin_results = []

                    for x, y in whatweb_result["plugins"].items():
                        if x not in WHATWEB_PLUGIN_BLACKLIST and y:
                            whatweb_plugin_results.append(f"{Fore.LIGHTYELLOW_EX}{x}{Fore.RESET}: {y}")

                    result_str = "\n".join(whatweb_plugin_results)
                    logger.info(f"{TAG} >> whatweb results for \"{whatweb_result['target']}\":\n{result_str}")
    finally:
        os.remove(path)


def add_host_entry(hosts, ip, hostname):
    entry_type = "ipv4" if isinstance(ip, ipaddress.IPv4Address) else "ipv6"
    hosts.add([HostsEntry(entry_type=entry_type, address=str(ip), names=[hostname])])
    hosts.write()
    logger.info(f'{TAG} Created hostentry: "{Fore.YELLOW}{hostname}{Fore.RESET}" -> "{Fore.YELLOW}{ip}{Fore.RESET}"')


def http_handle_unknown_host(url, redirect):
    try:
        # check wheter a host can be resolved, if not, add it to the hosts file
        target = urlparse(redirect).hostname or redirect

        try:
            socket.getaddrinfo(target)
        except:
            origin = urlparse(url).hostname or url
            hosts = Hosts()

            try:
                ip = ipaddress.ip_address(origin)
                add_host_entry(hosts, ip, target)
            except:
                for host_entry in hosts.find_all_matching(name=origin):
                    try:
                        add_host_entry(hosts, ipaddress.ip_address(host_entry.address), target)
                        return
                    except:
                        pass

                logger.error(f"{TAG} Failed to auto create hostentry")

    except:
        pass


def http_follow_redirects(url) -> str:
    resp = None

    try:
        resp = httpclient.HTTPClient().fetch(url, follow_redirects=False, validate_cert=False, raise_error=False)

        # only hanlde permanent redirects
        if resp.code in [301, 308]:
            redirect_loc = resp.headers.get("Location", None) or resp.headers.get("location", None)

            if redirect_loc:
                logger.info(f'{TAG} Redirect detected: "{Fore.YELLOW}{url}{Fore.RESET}" -> "{Fore.YELLOW}{redirect_loc}{Fore.RESET}"')
                http_handle_unknown_host(url, redirect_loc)
                url = http_follow_redirects(redirect_loc)
    except Exception as ex:
        logger.error(f"{TAG} Failed to follow redirect: {ex}")

    return url


def http_send_get(urls, description):
    http_client = httpclient.AsyncHTTPClient()
    reqs = [
        http_client.fetch(
            url,
            method="GET",
            request_timeout=0,
            validate_cert=False,
            raise_error=False,
            follow_redirects=False,
        )
        for url in urls
    ]
    return http_send_requests(reqs, description)


def http_send_requests(reqs, description):
    @gen.coroutine
    def fetch_and_handle():
        results = []
        waiter = gen.WaitIterator(*reqs)

        with tqdm(
            ascii=True,
            desc=description,
            unit=f" {Fore.YELLOW}requests{Fore.RESET}",
            total=len(reqs),
        ) as pbar:
            while not waiter.done():
                try:
                    response = yield waiter.next()
                except:
                    pass
                else:
                    results.append(response)

                pbar.update()

        return results

    return ioloop.IOLoop.current().run_sync(fetch_and_handle)


def http_discover_hvosts(url, wordlist):
    target = urlparse(url)
    initial_response = httpclient.HTTPClient().fetch(url, method="GET", follow_redirects=False, validate_cert=False, raise_error=False)
    initial_response_hash = xxh64_intdigest(initial_response.body)

    http_client = httpclient.AsyncHTTPClient()
    reqs = [
        http_client.fetch(
            url,
            method="GET",
            headers={"Host": f"{u}.{target.hostname}"},
            request_timeout=0,
            validate_cert=False,
            raise_error=False,
            follow_redirects=False,
        )
        for u in wordlist
    ]

    possible_vhosts = http_send_requests(reqs, f"{Fore.YELLOW}Scanning vHosts{Fore.RESET}")
    vhosts = []

    def filter_respone(response):
        if response.code == 200 and xxh64_intdigest(response.body) != initial_response_hash:
            return response.request.headers.get("Host", None) or response.request.headers.get("host", None)
        return None

    with ThreadPoolExecutor() as pool:
        for vhost in pool.map(filter_respone, possible_vhosts):
            if vhost:
                vhosts.append(vhost)

    return vhosts


def http_recon(host, port, https: bool = False):
    try:
        url = buid_url("https" if https else "http", host, port)
        url = http_follow_redirects(url)

        try:
            run_whatweb(url)
        except Exception as ex_whatweb:
            logger.error(f"{TAG} Failed to run whatweb: {ex_whatweb}")

        vhosts = http_discover_hvosts(url, get_wordlist("vhosts"))

        for vhost in vhosts:
            logger.info(f'{TAG} Discovered vHost: "{Fore.YELLOW}{vhost}{Fore.RESET}"')
            http_handle_unknown_host(host, vhost)
            http_recon(vhost, port)
    except Exception as ex:
        logger.error(f"{TAG} Failed to run HTTP recon: {ex}")


def init_http_lfi_scan(args):
    url = list(urlparse(args[0]))
    parameters = parse_qs(url[4])
    lfi_parameters = list(filter(None, [x if "LFI" in y else None for x, y in parameters.items()]))

    lfi_urls = {}
    lfi_count = 0

    for lfi in get_wordlist("lfi"):
        try:
            filename = Path(unquote(lfi)).name

            if filename not in lfi_urls:
                lfi_urls[filename] = []

            for lfi_param in lfi_parameters:
                url_copy = url.copy()
                new_parameters = parameters.copy()
                new_parameters[lfi_param] = re.sub("LFI", lfi, new_parameters[lfi_param][0])
                url_copy[4] = urlencode(new_parameters)
                lfi_urls[filename].append(urlunparse(url_copy))
                lfi_count += 1
        except Exception as ex:
            logger.error(f'{TAG} Invalid LFI path "{lfi}": {ex}')

    lfi_responses = {}

    async def scan_url_lfi(file, urls, pbar):
        http_client = httpclient.AsyncHTTPClient()

        async def fetch_website(http_client, u):
            try:
                response = await http_client.fetch(u, method="GET", follow_redirects=False, validate_cert=False, raise_error=False)

                if response.code == 200:
                    lfi_responses[file] = response.body.decode()
                    return True
            except:
                return False

        for u in urls:
            if await fetch_website(http_client, u):
                break

        pbar.update()

    with tqdm(
        ascii=True,
        desc="Scanning LFI's",
        unit=f" {Fore.YELLOW}requests{Fore.RESET}",
        total=len(lfi_urls),
    ) as pbar:

        async def scan_url_lfis(futures):
            return await asyncio.gather(*futures)

        asyncio.run(scan_url_lfis([scan_url_lfi(file, urls, pbar) for file, urls in lfi_urls.items()]))

    for file, content in lfi_responses.items():
        logger.info(f'{TAG} LFI "{file}":\n{content}')


def identify_by_banner_grabbing(host: str, port: int, print_banner: bool = False):
    try:
        sock = socket.socket()
        sock.settimeout(4)
        sock.connect((host, port))
        sock.send("\r\n\0".encode("ASCII"))
        banner_bytes = sock.recv(1024)
        sock.close()

        banner = banner_bytes.decode()

        if banner:
            if print_banner:
                logger.info(f"{TAG} >> {Fore.LIGHTGREEN_EX}Grabbed banner{Fore.RESET}:\n{Fore.LIGHTWHITE_EX}{banner}{Fore.RESET}")

            for service, regexes in BANNER_REGEXES.items():
                for rgx in regexes:
                    if rgx.match(banner):
                        return service
    except Exception as ex:
        logger.error(f"{TAG} Failed to run bannergrab: {ex}")

    return None


def indentify_http_service(host: str, port: int) -> bool:
    client = httpclient.HTTPClient()
    logger.getLogger("tornado").disabled = True

    try:
        if client.fetch(buid_url("http", host, port)):
            client.close()
            return True
    except:
        pass

    try:
        if client.fetch(buid_url("https", host, port)):
            client.close()
            return True
    except:
        pass

    client.close()
    logger.getLogger("tornado").disabled = False
    return False
